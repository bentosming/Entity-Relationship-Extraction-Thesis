\chapter{CERED}

In this chapter we will describe our process of generating \textbf{C}z\textbf{e}ch \textbf{R}elationship \textbf{E}xtraction \textbf{D}ataset (CERED). We will discuss various decisions that we made during this process and their impacts.


\section{Overview}

The objective is to use distant supervision to create a Relationship Extraction dataset for the Czech language. This section is a brief summary for easier orientation in this chapter. Each of these paragraphs is a teaser for one section of this chapter.

First we research available knowledge bases and Czech text corpora to determine which ones will best suit our purpose. We chose Wikimedia projects Wikidata and Czech Wikipedia.

Next we analyze how we will find mentions of Wikidata relations in Czech Wikipedia. We sketch out dataflow diagrams and we think about all the different complex aspects of this task.

We continue by choosing technologies that we use. Aware of the volume and other characteristics of chosen data, we choose Python as the main programming language, Spark as a way to speed up the computations and MorphoDita to deal with the specifics of the Czech language.

The questions and options that rose from the analysis get at least partially answered and decided during implementation. We tested different configurations and went through the data to determine what will work best. As a result, we generated CERED, or more exactly many different CERED0-4 in search for the best one to use for training in the second part of this thesis. 



\section{Data sources}

To be able to perform distant supervision we need to find suitable data - Czech text corpus and a knowledge base (Figure \ref{obr03:DSD}). In the first subsection, we will explain the requirements and constraints we have on such data and present our options. In the next two subsections, we will provide more information on the chosen ones.

\begin{figure}[h]\centering
\includegraphics[width=60mm]{./img//Diplomka diagramy-Distant supervision}
\caption{Distant supervision diagram}
\label{obr03:DSD}
\end{figure}

\subsection{Constraints and requirements}
The main constraint is quite straightforward, there has to be a nontrivial shared set of entities and relations mentioned in the text and stored in the knowledge base. We expect fact-based text to be more suitable then fiction literature. Therefore we will prefer encyclopedic or journalistic genre \todo{tak špatná věta}. One option is to focus on some subset of Czech National Corpus \footnote{https://www.korpus.cz/}, for example SYN2013PUB, SYN2009PUB, and SYN2009PUB are corpora of written journalism. The other option is to lean in the direction of encyclopedic text with Czech Wikipedia.

To the best of our knowledge, our options for knowledge base are limited to Wikidata or Google Knowledge graph \footnote{https://developers.google.com/knowledge-graph}.

We decided to use Czech Wikipedia and Wikidata, mostly because the intersection of information expressed in text data and in structured data seems promising because they are build on each other. Another advantage could be the multilingualism of Wikimedia projects, and therefore the transferability of this work will be higher. \todo{Další důvody.. že jde stáhnout? že není blackbox? lepší disambiguita}


\subsection{Czech Wikipedia}

Wikipedia is a multilingual online encyclopedia created and maintained as an open collaboration project by a community of volunteers as defined in \cite{wiki:wiki}. From our point of view, Wikipedia is a corpus of text with tagged topics of articles and some entity mentions. Czech Wikipedia contains approximately 440 000 articles and ranks top 30 across all the different language editions of Wikipedia.\footnote{As of March 2020 according to https://en.wikipedia.org/wiki/List\_of\_Wikipedias}

A dump of Czech Wikipedia is about 1,6GB and 770MB when compressed.

\subsection{Wikidata}

Wikidata is a knowledge base which acts as a central storage of the structured data of Wikimedia projects. Just like Wikipedia, this project is freely available and edited by users (and bots). It provides the option to query the database online (for small enough queries), but it is also possible to download the database in standard formats.

The database focuses on \defineterm{items}, which represent objects, entities, concepts, etc.  The first data collected in Wikidata were links to a multilingual version of Wikipedia articles on the same topic - on the same Wikidata item. Each item is assigned an identifier, prefix Q and a unique number, referred to as \defineterm{QID}. A label together with a description of an item should serve as a human readable identifier. Labels, descriptions and optional aliases are language dependant.

\defineterm{Properties}, another big concept of Wikidata, can be thought of as categories of items (\wikiitem{mother}{P25} implies a category of all mothers) or as relations between items (\wikiitem{Ron Weasley}{Q173998} has a \wikiitem{mother}{P25} \wikiitem{Molly Weasley}{Q3255012}). Each property has its \defineterm{PID}, an identifier consisting of a prefix P and a unique number, and a data type for a value it can be paired with (such as an item, string, url, number or media file). \todo{hezčí formátování wikiitemm}

Information about any item is recorded in statements. Statement is a key-value pair of a property and a value of prescribed data type. For example, for \wikiitem{Ron Weasley}{Q173998} there are seven statements about his siblings:
\begin{itemize}
\item \wikiitem{sibling}{P3373} \wikiitem{Ginny Weasley}{Q187923},
\item \wikiitem{sibling}{P3373} \wikiitem{Fred Weasley}{Q13359612},
\item \wikiitem{sibling}{P3373} \wikiitem{George Weasley}{Q13359613} and so on.
\end{itemize}  \todo{formating}

Wikidata project contains over 80 000 000 items, which raises \todo{jiná slova} requirements on technological resources so that we can work efficiently with such data. JSON dump of Wikidata takes 110GB of disk space or 37GB if bzip2 compressed.

\input{chap_03_analysis}

\section{Used technologies}
\todo{koukli jsme se na diagram a mysleli, že celé ve spark a tak}
We chose Python to be our main programming language. To be able to work faster with a bigger volume of data, we wanted to use a CPU cluster, which leads to Spark. To top it, we will use MorphoDiTa to work with the Czech language. We implemented a simple Streamlit app we used to comfortably view the results of our Spark queries.

In this section we will briefly introduce these technologies.
\chybi{Neřešit moc do detailu, cca dva odstavce o každém. }

\subsection{Python}
Python is probably the most popular programming language in the ML community. It is a high-level language with a wide range of libraries. Libraries as NumPy, Pandas and Spark enable fast and accesible computation. Tensorflow, scikit and PyTorch allow users to focus mostly on data and ideas in machine learning. Less known libraries help us with Wikipedia parsing (wikitextparser, mwparserfromhell) or easy-to-create web apps (streamlit).

\subsection{Spark}
Apache Spark framework provides query like API and runs on clusters. This way parallel computation can be implemented without actually implementing any parallelism. Therefore, Spark can boost the speed of computation as well as the available memory for the computation.


\subsection{MorphoDiTa}

MorphoDiTa \cite{Morphodita} (Morphological Dictionary and Tagger) is an open-source tool for morphological analysis of natural language texts. It is designed to work well on inflective languages and achieves state-of-the-art results for the Czech language. MorphoDiTa has a python package via which we can do all standard operation such as tokenization (splitting text into sentences and words) and lemmatization (disambiguation of the inflected form of a word).

\subsection{Streamlit}
Streamlit is a framework for creating simple web apps. It has minimal and practical API designed for users from the data science / machine learning community. \todo{typografie} We use this library to create a more pleseant way of viewing the generated dataset.

\input{chap_03_implemenation}


\section{Results}
\chybi{statistiky výsledného datasetu, ve kterém jsou zanechány všechny třídy + jedna "deploy" verze, ve které jsou i negative mentions}

