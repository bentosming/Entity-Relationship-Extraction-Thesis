\section{Analysis}
\label{text:Analysis}
The process of creation of CERED is mostly an attempt to execute the first two parts of the pipeline we mention in the 
\nameref{sec:relation_extraction_pipeline_proposal} section. To the best of our knowledge, there is no suitable entity linking tool for Czech. There are tools for named entity recognition that we could theoretically use to our advantage if we decided to focus on named entities only.

Therefore we need to find a way to get to similar results as the first stages of the pipeline would get. We do not expect that our CERED generator will be as powerful as the respective dedicated tools would be. We will not try to create general entity recognition and linking tools - on the contrary, we will exploit any extra information that chosen Wikimedia projects provide.

There are several aspects that we need to think through. We introduce them in the following list for better orientation. A subsection in this section is dedicated to each of those aspects. \todo{úplně odstranit ten list?}
\begin{itemize}
 \item  Dataflow - we chose Wikidata and Czech Wikipedia, but we did not discuss how to connect them and what exactly should be the outcome so that we can proceed to locating mentions.
 \item  Entity Matching - suppose we collected a piece of text together wit a set of entities that could be mentioned in the text. The process of entity matching attempts to mark words in the text that mention an entity.
 \item  Wikilink Mentions - Wikilinks are (mostly) human labeled entity mentions. Utilizing them is the closest we can get to a supervised dataset without actually supervising the dataset.
 \item  Relation Matching - entities are matched in the Wikipedia texts, relationships are extracted from the Wikidata dump, we use distant supervision assumption to locate relation mentions in the texts.
\item Relation Inventory - we generated CERED, but the relation inventory is overly diverse. Moreover, the dataset is extremely unbalanced - the number of mentions per relation varies. And no “no relation” relation is obtained.
 \item  Result Evaluation - every time we generate CERED during development, we need to evaluate its quality. We propose methods for this evaluation. 
\end{itemize}




\subsection{Dataflow}

We start the whole process of creating our dataset with two files. The first file is a Czech Wikipedia dump. It is a collection of articles where each article has its title, id and text. And the other file is a Wikidata dump. 

The simplest way of processing those files would be to process them separately and thus obtaining sentences on one side and relationships (a relation type with two items) on the other. This approach comes with a clear disadvantage. We would lose any additional information about the sentences that could be potentially useful (for example article title might be helpful to determine which items are mentioned in the sentence). 

To solve this we could precompute something for each article and attach it to each sentence (article title, all Wikilinks in the article, etc.) risking a massive increase in required capacity to work with such data. On a similar note, we would probably process Wikidata to store item names (labels and aliases) for each relationship, worsening the situation even further.

\begin{figure}
\centering
\subfloat[Uninformed approach]{\includegraphics[width=0.4\textwidth]{./img/Diplomka diagramy-uninformed} \label{obr:uninformed}}
\qquad
\subfloat[Informed approach]{\includegraphics[width=0.6\textwidth]{./img/Diplomka diagramy-informed} \label{obr:informed}}
\label{obr:informedvsuninformed}
\end{figure}



We decided to update the dataflow to address those issues. We will preprocess Wikidata dump to contain only the data we will use. We will refer to this processed version of Wikidata as \defineterm{custom Wikidata}. An item will be kept only if it has a Czech name and we will significantly reduce its statements: we will keep the title of its Czech Wikipedia article and create a list of (QID,PID,QID) triples - \defineterm{QPQ}, representing statements that contain information about relations between this item and the other items. This way, we have all the necessary information - article title to be able to connect an article to an item, names for each item to be able to find mentions of items and finally QPQ triples to connect relations and sentences. Moreover, custom Wikidata size is closer to traditional RAM size. Therefore we could for example load item names into memory, which will come in handy during the implementation.

One approach to finding item mentions in text could be called uninformed (Figure \ref{obr:uninformed}). We could assume that any item can be mentioned in any sentence. This approach seems to have two issues: the computation would likely take quite some time but mainly we expect a huge amount of ambiguous mentions. An example of this ambiguity that we see as problematic might be children named after their parents. In this case, not only that the entities might get confused but also if we then assign the relation, we might easily confuse a sentence mentioning a spouse relation for a parent relation, which, unfortunately, is extremely challenging to solve. 

On the other side, we can use the extra information that Wikimedia projects provide and opt for a more informed approach. A diagram of this approach is captured in \ref{obr:informed} The topic of most Czech Wikipedia articles is a Wikidata item, therefore this item is nearly certainly mentioned in the article. Some Wikidata statements were based on relevant articles and thus it seems rational to expect items, that are related to the main item of an article, to be mentioned. We decided to look only for a tiny subset of all Wikidata items in each article - \defineterm{candidate items}. As we just discussed, if an article is based on an item, then this item and all items, that are connected to it by a statement, are considered candidate items.

Czech Wikipedia maintains a wbc entity usage table, which contains information about which article uses which item. If we use this table, we are able to obtain a list of items, that should be mentioned in an article, let us call this list a \defineterm{wbc candidates}. A wbc candidate is at the same time a candidate item.

We might consider adding even a second level of relatives (items related to items that are related to the main item) but the branching factor might be relatively high and cause unwanted ambiguity. Consider an instance item like a specific country, all countries would be second level relatives and thus a candidate item. Since countries tend to be of a certain type (kingdom, republic, state etc.) there might be simply the type or some other more general name amongst their names (\todo{typografie}Q30 United States of America are also known as America or United States) and more countries might share this name.

So far we mostly discussed the advantages of the proposed informed approach, mainly a hope for higher precision, specifically higher precision for item mentions. We should elaborate on some disadvantages as well. We are not trying to fully do entity linking. In the end we will only use item mentions, if the following condition holds: there are two entity mentions in one sentence and there exists a QPQ that connects them. It is debatable whether we need an informed approach to increase relation mention precision. The improbability that this condition will be fulfilled for false-positive item mentions might in fact be sufficient.

One more way to locate item mentions is trough \defineterm{Wikilinks}. A Wikilink links a page to another page within the same-language Wikipedia. First additional information this brings is simply the item mention (if the linked page or article has its main item). We can also consider the textual part of the link to be another name for the linked item. The quality and suitability of this name are to be examined and if we will find these names useful, they can be added to the item names we use.



sice by šlo neparalelně, ale co rychlost?

Mluvit o tom, proč nejdřív najdeme, co v článku hledat, pak to nasekáme na věty, pak matchujeme. Zmínit, kolik je jiných možností, že teoreticky by šlo ještě před rozsekáním na věty dělat entity linking ...

Detailně popsat, co kdy kam poteče + diagram


\subsection{Entity matching}

We have text on one side, gathered candidate items on the other and our goal is to find occurrences of these items in the text. We call this process \defineterm{entity matching} and each found occurrence is an \defineterm{entity mention}.

No matter how the matching will be done, it seems always beneficial to start the process with some text preprocessing. Quite a lot of changes need to happen even if some seem like little details. We will separate this preprocessing into a wiki specific part, lexical analysis and the last part is devoted to lexical analysis on standalone noun phrases.

When we eventually proceed to entity matching, there is a wide spectrum of complexity we might aim for. We are not trying to create a strong sophisticated tool for entity recognition and linking. We will describe some of those complexity tears and choose the right method for our use case.



 

\subsubsection{Wikipedia parsing}

Wikipedia parsing starts with an article in Wikitext and produces human-readable plain text - \defineterm{clean text}. We should keep track of positions of Wikilinks from the Wikitext in the clean text.

Wikipedia is written in Wikitext (Wiki markup, Wikicode). This markup provides all usual functionalities such as determining the layout or fonts and enables commonly-used concepts like lists, links, media file insertion, or tables, and some more wiki specific concepts like infoboxes.

We plan on using one or even a combination of existing Wikitext parsers since each of them provides different functions.\footnote{https://www.mediawiki.org/wiki/Alternative\_parsers} Therefore the parsing itself is not too troublesome. 

One problem that needs to be addressed is what should we consider to be a valid text. For example, it is not clear how to work with tables. From one point of view, if we convert a table into an unstructured text, the text will not be a regular text in terms of sentence structure. From a different point of view, an unstructured text obtained by converting a table still contains information, that human readers will likely decode. Moreover, tables and other structured data tend to contain a lot of information. This will likely cause problems because we want to concentrate on sentence-like data, not just for example tuples of data that for example a table of athletes might provide (tuples of persons and countries). This kind of data might significantly damage the quality of CERED.

The elimination of all Wikipedia content, that is too structured or generally, not enough sentence-like, but at the same keeping as much as possible will be addressed later in \nameref{sec:wikiperia_parsing_implementation}. That way we can see the consequences of the eliminated and kept content.


\subsubsection{Lexical Analysis}
For our purpose keeping the text in long sequences of characters (such as an article) is not the best format. We need to parse those sequences into smaller tokens, such as words and sentences. A tool that addresses such tasks is usually called a \defineterm{tokenizer}.
Tokenization is a process that aims to split text (sequence of characters) into separate tokens. Token is a term that generalizes the term word and often words in text are the same as tokens. In english, \vuvozovkach{aren't}, in Czech \vuvozovkach{mohu-li} are likely considered one word but two tokens. On the contrary \vuvozovkach{M*A*S*H} or \vuvozovkach{email@email.com} might not be considered a word, but should be considered exactly one token each. Naive tokenizer might just simply split on non-alphanumeric characters. But if the tokenizer is supposed to actually perform well and recognize sentences, a more sophisticated tool is needed. 
Since we wish to work with the language even further, we might want to be able to tell the non-inflexed form of a token, moreover we might want to assign some unique idintifier to such form in case it has some homographs (words that are spelled the same way). Such identifier is called a \defineterm{lemma}. The collection of tokens that share the same lemma is called a \defineterm{lexeme}. 
We will outsource handling text to a Czech tokenizer called MorphoDiTa, that achieves state-of-the-art results for the Czech language. Using such a tool, we can convert clean text into sentences made up of tokens and we even obtain the lemma and lexeme of each token.

\subsubsection{Lexical Analysis on Names}
Tokenizers (and lemmatizers) are usually trained to perform well on sentences and might be inaccurate on noun phrases when they stand alone, as entity names do. If we were determined to tokenize them, a simple trick like constructing a sentence with the name in it and tokenizing this sentence can partially solve this problem. Such a sentence that would be grammatically correct and not semantically terrible could be something like “This is /name/”, but realistically, this sentence was quite likely not at all common in the training process of MorphoDiTa. Some foreign words can be erroneous as well and keeping their original form might be the only easy way around it.

\subsubsection{Matching methods}
Entity matching can be done with various degrees of sophistication. We provide a short overview of those degrees (1 -- 4). 1 and 2 do not require any knowledge of the language they work with, 3 and 4 are language dependant. For simplicity, we will assume that we are only looking for mentions in one sentence at a time unless written otherwise. We also include an example of such matchings to demonstrate how successful we are likely to be.

\begin{figure}

\chybi{Lord Voldemort (Tom Rojvol Raddle, Voldemort, Pán zla)}

\caption{First paragraph from Czech Wikipedia page about Lord Voldemort Q176132, also known as Tom Rojvol Raddle, Voldemort, Pán zla (Dark Lord) }


\end{figure}
\todo{značka pro překlad v caption} 

\todo{Formátování nadpisů }

\subsubsub{1 String equality} Definitely the easiest method of entity matching. This method is based on a simple substring check which is later extended with additional functionality. In more detail, for each entity, we have multiple name variants and for each of those names, we check whether the name is a substring of the sentence.

We still need to work with letter cases. Named entities should have fixed letter cases and no additional processing is needed in most cases. In other cases, an established name for a named entity might be written with the lowercased first letter (Weasley family (Q716534) has Czech names 'Weasleyovi' /the Weasleys/ but also rodina Weasleyových /Weasley family/, but there are examples where those different names are completely different (\todo{český} Elizabeth II and Queen of England would be translated to Alžběta II. and královna Anglie). If we consider entities, that are commonly written with lower case (\todo{příklady jako škola, tužka}), the sentence now needs to be preprocessed so that for example the first letter is not capital. Moreover, there is no guarantee that common names will be lowercased in Wikidata. To conclude, nearly nothing can be assumed about the case of letters, and therefore one of the following solutions needs to be implemented: everything can be converted to one chosen letter case or some more sophisticated attempts at predicting, which words can have more version in terms of letter cases.

Another problem that we may encounter is how to properly handle spaces. We will list some troublesome examples and accept the fact that not everything can be done perfectly. J. K. Rowling has J.K.Rownling as one of Wikidata names, confirming that both versions might appear in written text, but not all entities with similar name type have all space-variants listed in names. We assume that spacing around the ‘-’ \todo{-} character might vary.

It is also not clear if word order in entity names is fixed (or at least almost always fixed). Even the simple reversion in name, that is sometimes used, will affect the performance of this method (J. K. Rowling and Rowling, J. K.).  Even cases where the name is divided by for example apposition into two separate parts might exist.

The greatest weakness of this method is its inability to recognize entities if their name is inclined. To emphasize how many words are not in the same form as their lemma in Czech text, we colored them in the sample text. \todo{vybarvit a dát link} We elaborated on Czech language in \todo{link} CHAPTER XX, but just for simplicity - in English the verb to be has many different forms (am, are, were, was, would and so on), all nouns and verbs in Czech behave like this, quite often with many more forms.

\subsubsub{2 String similarity (approximate string matching)} String similarity is still based on simple string manipulation, no vocabulary or other language knowledge is necessary. The goal is to find entity mention, even if its name is a little altered in the sentence. This alternation can include all of the issues listed for the previous method - cases of letters, spacing, word order, and word forms, but even better, it might help in cases, that we did not anticipate.

There are many metrics describing string similarity. Some could cope better with word order issues, some with word forms, some with spacing. We will not test all of them for our usecase, but still find it useful to mention them since in other than the Czech language, some might work well.

\defineterm{Edit distance}. First category of string similarity metric is based on edit distance:

Levenshtein distance is the minimum number of edits (additions, deletions, and substitutions of a character) to get from one string to the other. As a metric the ratio of Levenshtein distance and of the sum of the lengths of the strings can be used. This metric, unsurprisingly, deals well with mentions that are close to the names when it comes to the amount of edits needed, so mentions differentiating in a word form, different spacing, or letter casing will could be considered a match.

Damerau–Levenshtein distance is very similar to the previous, but a transposition of two adjacent characters is considered an edit. We might argue that some Czech words tend to transpose the last characters in different word forms and thus this metric could work better for those forms, but there might be a higher risk of false positives.

One more thing to mention about edit distances is that they count the distance of two string, in our case of a name and of a substring of a sentence, because we do not expect an entire sentence to be entity mention. Therefore we need to decide on a logic for choosing substrings of a sentence to count the distance on, without diving too deep into this, the time complexity (even though both the sentence and the name length is relatively small) can be high if we consider the amount of data we need to process. (Let $s, n$ be lengths of a sentence and a name, there are $s^2/2$ substrings and each edit distance can be computed in $sn$, leading to $s^3n$). \todo{vynechat}

\defineterm{Token based}. Another category is based on tokens. For those metrics, we can either use the tokenized sentences (by an actual tokenizer) and try to tokenize names, so that the format matches, or we can use naive tokenization like removing non-alphanumeric character and splitting on spaces.

The tokenization converts both the sentence string and the name string into a set of tokens ($S, N$ respectively), so metrics that work with sets can be utilized: Intersection over union is computed as $|S \cap N| / |S \cup N|$. Any other set similarity measure can be used. 

Token based metrics - due to their set nature - ignore the order of tokens and therefore could solve issues with mentions in which the word order is not the same as in the name. On the other side, an increase in false positives is to be expected and some additional postprocessing is needed to determine which token in the sentence should be considered a mention if the token was used in the sentence multiple times. We also feel obligated to mention that once again, we are not trying to find the similarity of a sentence and a name, but of a substring of a sentence and a name. This leads to the idea of removing all tokens, that are in a sentence and would worsen the metric and in result modifying the formula for intersection over union to  $|S \cap N| / |N|$.


\defineterm{3 Sequence based}. Just to be a bit more comprehensive we include another type of metrics - sequence based, even though we doubt that it is the best approach for entity matching. They ignore words as wholes and we do not see any advantage of those metrics for our use case. 

Ratcliff-Obershelp similarity finds the longest common substring that is longer than some limit and recursively does so for the non-common parts of strings. The result is based on the ratio of (double the) length of common parts and overall length. 

Bigram (or n-gram) intersection over union which converts both strings into a set of n-grams (n adjecant characters) and performs intersection over union. This time reducing a sentence into a substring is not that straight forward and would require additional attention.


\subsubsub{Morphological analysis}
Moving on from methods that are mostly unaware of the language they work with, we will finally use the morphological analysis we mentioned earlier.

With lemmas of both the sentence and the name, we can use any metric from the previous subsection on string similarity (joining the lemmas on the space char if the metric expect only two input strings).

If we decide to keep the names in their original form (tokenization on them can be error prone, as we already explained), we can try to use the correct form of the tokens in the sentence. For each lemma, we get a set of all its possible forms - a lexeme, now we can modify the previous metrics to work with lexemes instead of tokens. 


\begin{figure}[h]\centering
\includegraphics[width=140mm]{./img/Diplomka diagramy-Detailed_text}
\caption{Matching with morphological analysis}
\label{obr:DiagramTextDetail}
\end{figure}


\subsubsub{4 Advanced concepts}

A proper entity matching (either in named entity recognition or entity linking) might be expected to recognize entity mention even if the entity is not mentioned explicitly by its name. Pronounces should be assigned an entity they represent (if they do) and other nouns as well. In languages like Czech where the subject of a sentence is often omitted the entity mention is even less obvious but still present. Since the topic of this thesis is not entity matching, we will not debate techniques to achieve this level of matching neither will we implement them.

\todo{mezera}
\vspace{2mm}
Sfter looking at the results, we used a simple metric but we still find it useful to keep this summarization of different string metrics as part of this thesis. A detailed description of the matching method CERED was generated with is written in XX \todo{YY}.

\todo{
Napsat, že v aj dělají často jen exact modulo zkratky a malé přípony, což tady nejde.

Ukázat nápady se sebráním linků z wikipedie a zavrhnout to

připomenout, jak moc se dá čeština skloňovat
říct, že nemá nejspíš smysl snažit se najít jen validní tvary, protože stejně v textu nejspíš nebudou nevalidní

asi mluvit o word order? a možná i implementovat

Říct, že jako kontrolní dataset budou přímo z linků}

\subsection{Wikilink Mentions}

As we already mentioned, from our point of view Wikilinks are entity mentions created by Wikipedia editors. The text part of the link can in theory be anything providing us with some more advanced examples of entity linking, that our matching methods cannot perform.

V pátém díle McGonagallová říká, že předmět vyučuje již 39 let

We want to enable the users of CERED to distinguish relation mentions that were created based on two Wikilinks from all others. Since this data is not fully supervised and the word supervised is used often \todo{přetížený} (semi-supervised, distant-supervised, etc.) we decided to call it \defineterm{silver}, because they are not of the optimal quality that is usually labeled gold, but they are the best we can get.


\subsection{Relation Matching}
If a sentence contains two entity mentions that are related, chances are that the sentence in fact does express their relationship and thus is a relation mention. This concept is called \defineterm{distatnt supervision assumption} and can be also formulated in the following way: If two entities participate in a relation, all sentences that  mention these two entities express that relation. This assumption is commonly used, even though it is clearly not correct, because it is easy to use it. To tell how often this assumption is violated is labor-intensive, luckily research has been done on this topic. In the \cite{nytdistant} \todo{link} the distant supervision assumption is compared to an \defineterm{express-at-least-once assumption} which states that if two entities participate in a relation, at least one sentence that mentions these two entities might express that relation. 

They sampled 600 relation mentions from two corpora, both created by distant supervision on Freebase (knowledge base commonly used before Wikidata took over) and two text corpora - Wikipedia articles and the New York Times corpus. These 600 samples represented three different relation types (nationality, place of birth, and contains) and were sampled so that there were 100 samples of each type in each corpus. We include their results in table XX \todo{table} They concluded that Wikipedia is a very specific type of text corpora, because articles are centered around entities. We believe that the reasoning can be extended with the fact, that freebase contained information from Wikipedia infoboxes, and those infoboxes were created based on the textual information. 

For the authors the results signalized that a more sophisticated tool is needed instead of relying on the distant supervision assumption. We acknowledge that such a tool is needed but at the same time we believe that in our case, where we create CERED based on Wikipedia and Wikidata, the precision they estimated is sufficient. We also assume that Wikidata project is more suitable for this task than was Freebase.

We want to mention that we build CERED to easily fit into the modern deep learning models and to be as simple as possible. Therefore, the main piece of text we use is a sentence, it might seem intuitive, but it has one downfall. If the relationship extractor was to be used on a real text not to determine where some relations are mentioned but to provide a summary of relations expressed by the text as a whole, some information will be lost. \todo{najít článek nebo někam přesunout?}

\subsection{Relation Inventory}
In the \nameref{chap:datasets} chapter some examples of Relationship classification datasets were introduced. The creators of those datasets claim that in the creation process they first decided on the relation inventory (relation types).  Creating the relation inventory seems to be the straight forward and rational approach and we wanted to create such inventory before actually implementing the CERED generator. We stumbled upon the following issues.

\subsubsub{Issue 1}Wikidata relation inventory (properties in Wikidata terminology) is an order of magnitude larger compared to the traditional relationship extraction datasets and handpicking our inventory is overwhelming. We even considered reducing the size of this inventory by creating our own relations that would combine Wikidata relations (parent would be the combination of mother and father relations). 

By restricting the Wikidata inventory to just some properties we reduce the size of CERED and if we choose the inventory without considering the number of mentions per relationship, the best-represented relations could be omitted.

\subsubsub{Issue 2}Knowledge bases, in general, do not contain negative relations (such relations that could be easily mapped to the \relationtype{no relation} or \relationtype{other relation}), but for relationship extraction negative mentions are essential. If we generate mentions using all properties we can later decide which relationships will be in the inventory and the rest of them relabel to \relationtype{other relation}. If we were to assign all tuples of entity mentions that share a sentence and are not related as \relationtype{no relation}, we could increase the noise in CERED because not all relationships are in Wikidata and therefore some of the \relationtype{no relation} mentions could in fact be a positive mention. The ratio of negative and positive mentions in the two bigger datasets we introduced in the \nameref{chap:datasets} chapter were approximately 80\%.

While curating the inventory, we should keep in mind that we are not just choosing the relations but also their representations and we need to attempt to fulfill the three following requirements to the best of our abilities:
\begin{itemize}
\item Each relation needs to be represented enough.
\item The more balanced relation representation sizes the better.
\item There should be enough negative mentions and their negativity should be assured. \todo{reformulovat}

\end{itemize}

\subsection{Result Evaluation}
The most challenging aspect of working with Czech Wikipedia and Wikidata is their size and diversity. To the best of our knowledge there is no strictly followed guideline when it comes to editing either the articles or item information. Just converting Wikipedia dump to clean text will be challenging due to user defined templates and other constructs we might be unaware of. On the other side names in Wikidata can be too general (like someone's first name) and create false entity mentions. 

We prepared several metrics and methods to measure the quality of the implemented generator and we will list them in the table \ref{tab:qualityEval}. Unfortunately going through the article and seeing what did and did not get recognized as a mention is still the most powerful method, therefore we decided to develop a simple app that can prettify the output of the generator and makes the process of looking at the results less painful. \todo{příliš neformální?} 

\begin{figure}

\begin{tabular}{p{0.05\linewidth}p{0.8\linewidth}}

FA & The overall count of found mentions - to some extent the greater the better - at least in the beginning with naive matching methods. With more sophisticated matching, this number should increase. On the other side, this measure will decrease with more higher precision. \\ 
FA & If the distribution of the amount of found mentions (both the entity mentions before relation matching and the relation mentions) over some domain is peculiar (contains abnormalities such unexpected peeks) the quality of not only those mentions might be lower. The domain can be anything from the following or even multiple of them at the same time:
sentence, article, relation, entity, sentence number (the order of the sentence in an article), entity pair. \\ 
NA & Checking the anomalies detected by the previous methods.  \\ 
NA & Checking an article and realizing what did and did not get matched. \\ 
FA & Some articles could be labeled by hand and test could be created.\\

\end{tabular} 

\caption{Quality evaluation methods and the potential for automatization (Fully Automatic, Not-Automatic)}
\label{tab:qualityEval}
\end{figure}


\subsubsection{Viewer}
\todo{co je viewer}
The purpose of this viewer is to present the results in a more graphical way to fasten and pleasant the process of checking them, which will be done often and will take a significant amount of time.

We want the viewer to be able to show the statistics about the generated data, that were collected during the generation, the configuration that was used to generate the dataset, and mainly to show an article with the found mentions (entity and relation). 

Streamlit made implementing such viewer quite easy and we used spaCy’s \todo{footnote} visualizations. \todo{přestěhovat někam dopryč tento odstavec}

\todo{screen viewertu k implementaci}





%\begin{figure}[p]\centering
%\includegraphics[width=140mm, height=117mm]{./img/Corpus_diagram}
%\caption{Zjednodušený diagram výroby korpusu}
%\label{obr03:Nhust}
%\end{figure}







\tikzset{lines/.style={draw=none},}





\begin{figure}
\centering
\begin{subfigure}[t]{.5\textwidth}
  \centering
\begin{tikzpicture}
  \tikzstyle{every node}=[font=\tiny]
\pie [rotate = 180, hide number,radius =2,style={lines}]{
8e+01/no\_relation,
1.3/ ,0.54/ ,0.71/ ,0.031/ ,0.16/ ,0.25/ ,0.16/ ,0.27/ ,
0.11/ ,0.42/ ,0.12/ ,0.14/ ,0.33/ ,0.43/ ,2.6/ ,0.21/ ,
0.78/ ,0.14/ ,0.32/ ,0.26/ ,0.33/ ,0.7/ ,0.097/ ,0.21/ ,
0.77/ ,0.05/ ,0.057/ ,0.097/ ,0.37/ ,2.0/ ,0.63/ ,0.3/ ,
0.28/ ,0.14/ ,0.22/ ,0.24/ ,0.45/ ,0.068/ ,0.098/ ,0.46/ ,
3.6/ 
}
\end{tikzpicture}
  \caption{TACRED}
  \label{fig:corridors_input}
\end{subfigure}%
\begin{subfigure}[t]{.5\textwidth}
  \centering
\begin{tikzpicture}
  \tikzstyle{every node}=[font=\tiny]
\pie [rotate = 210, hide number,radius =2,style={lines}]{
1.7e+01/Other,
1.2e+01/Cause-Effect,8.8/Product-Producer,9.1/Entity-Origin,6.2/Instrument-Agency,
1.2e+01/Component-Whole,6.8/Content-Container,1.1e+01/Entity-Destination,8.6/Member-Collection,8.4/Message-Topic
}
\end{tikzpicture}
  \caption{S10T8}
  \label{fig:corridors_before}
\end{subfigure}


\begin{subfigure}[t]{.5\textwidth}
  \centering
\begin{tikzpicture}
  \tikzstyle{every node}=[font=\tiny]
\pie [rotate = 180, hide number,radius =2,style={lines}]{4.5/ ,1.9e+01/other,0.84/ ,0.47/ ,2.7/ ,3.0/ ,0.4/ ,0.89/ ,6.7/územní jednotka,0.64/ ,2.2/ ,2.0/ ,0.29/ ,8.4/podcelky,5.9/je podtřídou (čeho),3.1/ ,0.86/ ,0.44/ ,0.48/ ,0.37/ ,0.91/ ,2.9/ ,1.8/ ,1.4/ ,0.89/ ,0.38/ ,2.7/ ,2.0/ ,4.6/ ,0.39/ ,0.34/ ,0.89/ ,0.29/ ,0.34/ ,1.2/ ,0.29/ ,0.67/ ,1.3/ ,0.4/ ,1.4/ ,0.32/ ,0.42/ ,1.1/ ,0.39/ ,0.5/ ,0.29/ ,0.5/ ,0.56/ ,0.32/ ,0.76/ ,1.3/ ,0.55/ ,0.58/ ,0.64/ ,0.45/ ,0.35/ ,0.31/ ,0.32/ ,0.28/ ,0.4/ ,0.63/ ,0.28/ ,0.39/ ,0.29/ }
\end{tikzpicture}
  \caption{CERED2}
  \label{fig:corridors_input}
\end{subfigure}%
\begin{subfigure}[t]{.5\textwidth}
  \centering
\begin{tikzpicture}
  \tikzstyle{every node}=[font=\tiny]
\pie [rotate = 210, hide number,radius =2,style={lines}]{4.8/ ,1.3e+01/je,3.7/ ,0.25/ ,1.5/ ,0.027/ ,5.9/profese,1.9e+01/země,3.7/ ,2.7/ ,6.7/územní jednotka,0.0079/ ,0.44/ ,0.14/ ,0.053/ ,0.75/ ,0.66/ ,3.4/ ,0.0036/ ,0.23/ ,0.066/ ,0.27/ ,0.028/ ,1.2/ ,0.38/ ,0.093/ ,0.97/ ,1.3/ ,0.1/ ,0.1/ ,0.15/ ,0.057/ ,0.1/ ,0.097/ ,3.2/ ,0.16/ ,0.034/ ,0.48/ ,1.1/ ,0.048/ ,0.021/ ,0.081/ ,0.018/ ,0.072/ ,0.34/ ,1.3/ ,0.027/ ,0.99/ ,0.13/ ,0.043/ ,0.00034/ ,0.13/ ,0.15/ ,0.78/ ,0.036/ ,0.018/ ,0.07/ ,0.57/ ,0.0046/ ,0.23/ ,0.15/ ,0.26/ ,0.048/ ,0.00075/ ,0.0016/ ,0.025/ ,0.7/ ,1.2/ ,0.21/ ,0.048/ ,0.048/ ,0.15/ ,0.13/ ,0.25/ ,1.5/ ,0.18/ ,0.27/ ,0.18/ ,0.0024/ ,0.22/ ,0.18/ ,0.12/ ,0.022/ ,0.11/ ,0.17/ ,0.079/ ,0.39/ ,0.015/ ,0.038/ ,0.027/ ,0.12/ ,0.33/ ,0.21/ ,0.13/ ,0.079/ ,0.031/ ,0.091/ ,0.37/ ,0.057/ ,0.058/ ,0.083/ ,0.13/ ,0.016/ ,0.52/ ,0.0058/ ,0.0083/ ,0.031/ ,0.0073/ ,0.11/ ,0.32/ ,1.1/ ,0.1/ ,0.043/ ,0.024/ ,0.028/ ,0.0036/ ,0.067/ ,0.061/ ,0.029/ ,0.094/ ,0.0057/ ,0.007/ ,0.031/ ,0.076/ ,0.07/ ,0.13/ ,0.076/ ,0.057/ ,0.11/ ,0.32/ ,0.019/ ,0.029/ ,0.25/ ,0.0083/ ,0.088/ ,0.018/ ,0.49/ ,0.098/ ,0.011/ ,0.019/ ,0.057/ ,0.054/ ,0.039/ ,0.0043/ ,0.021/ ,0.0035/ ,0.063/ ,0.1/ ,0.081/ ,0.012/ ,0.22/ ,0.0055/ ,0.11/ ,0.11/ ,0.061/ ,0.0067/ ,0.0073/ ,0.021/ ,0.027/ ,0.12/ ,0.11/ ,0.009/ ,0.01/ ,0.0086/ ,0.012/ ,0.17/ ,0.19/ ,0.00021/ ,0.047/ ,0.0053/ ,0.018/ ,0.066/ ,0.026/ ,0.17/ ,0.017/ ,0.073/ ,0.029/ ,0.021/ ,0.041/ ,0.019/ ,0.11/ ,0.017/ ,0.041/ ,0.017/ ,0.02/ ,0.0027/ ,0.031/ ,0.044/ ,0.0079/ ,0.019/ ,0.021/ ,0.096/ ,0.053/ ,0.11/ ,0.041/ ,0.11/ ,0.00034/ ,0.025/ ,0.0029/ ,0.0038/ ,0.059/ ,0.0044/ ,0.013/ ,0.014/ ,0.066/ ,0.046/ ,0.021/ ,0.01/ ,0.011/ ,0.032/ ,0.031/ ,0.063/ ,0.038/ ,0.0059/ ,0.22/ ,0.065/ ,0.0071/ ,0.028/ ,0.036/ ,0.049/ ,0.15/ ,0.11/ ,0.022/ ,0.022/ ,0.00041/ ,0.021/ ,0.0098/ ,0.018/ ,0.0088/ ,0.00062/ ,0.031/ ,0.008/ ,0.057/ ,0.003/ ,0.031/ ,0.0062/ ,0.0014/ ,0.068/ ,0.043/ ,0.0085/ ,0.15/ ,0.0029/ ,0.0027/ ,0.011/ ,0.029/ ,0.029/ ,0.0049/ ,0.0065/ ,0.0026/ ,0.0021/ ,0.00014/ ,0.00014/ ,0.011/ ,0.027/ ,0.0054/ ,0.0088/ ,0.022/ ,0.0078/ ,0.015/ ,0.0088/ ,0.0023/ ,0.00068/ ,0.053/ ,0.0027/ ,0.016/ ,0.0013/ ,0.0031/ ,0.00075/ ,0.012/ ,0.0033/ ,0.0096/ ,0.027/ ,0.0062/ ,0.0083/ ,0.0074/ ,0.011/ ,0.0038/ ,0.0051/ ,0.01/ ,0.006/ ,0.0081/ ,0.0044/ ,0.017/ ,0.00062/ ,0.0049/ ,0.019/ ,0.0059/ ,0.00041/ ,0.0046/ ,0.0015/ ,0.0071/ ,0.075/ ,0.013/ ,0.018/ ,0.0062/ ,0.0016/ ,0.015/ ,0.0026/ ,0.0079/ ,0.001/ ,0.0033/ ,0.02/ ,0.0031/ ,0.0053/ ,0.0044/ ,0.0091/ ,0.016/ ,0.0017/ ,0.0016/ ,0.0025/ ,0.00055/ ,0.00068/ ,0.00048/ ,0.0043/ ,0.0041/ ,0.0035/ ,0.004/ ,0.0012/ ,0.0012/ ,0.0015/ ,0.00027/ ,0.0011/ ,0.0029/ ,0.0024/ ,0.007/ ,0.00055/ ,0.0054/ ,0.0058/ ,0.0014/ ,0.00096/ ,0.0019/ ,0.0049/ ,0.028/ ,0.0023/ ,0.006/ ,0.0038/ ,0.0041/ ,0.0049/ ,0.0053/ ,0.0014/ ,0.0071/ ,0.0023/ ,0.01/ ,0.01/ ,0.0012/ ,0.0018/ ,0.0023/ ,0.0082/ ,0.0045/ ,6.8e-05/ ,0.00014/ ,0.0051/ ,0.0013/ ,0.0069/ ,0.004/ ,0.004/ ,0.0051/ ,0.01/ ,0.0057/ ,0.0018/ ,0.0023/ ,0.00041/ ,0.003/ ,0.00027/ ,0.0021/ ,0.0014/ ,0.00027/ ,0.0031/ ,0.0017/ ,6.8e-05/ ,0.0055/ ,0.00062/ ,0.0026/ ,0.0029/ ,0.0052/ ,0.00027/ ,0.0053/ ,0.01/ ,0.005/ ,0.0015/ ,0.0014/ ,0.004/ ,0.0012/ ,0.00048/ ,0.0068/ ,0.0018/ ,0.0013/ ,0.007/ ,0.00034/ ,0.0014/ ,0.0013/ ,0.0016/ ,0.0046/ ,0.00068/ ,0.00068/ ,0.00062/ ,0.00048/ ,0.0028/ ,0.00014/ ,0.00041/ ,0.0027/ ,0.00021/ ,0.00027/ ,0.0029/ ,0.00034/ ,0.0013/ ,0.00041/ ,0.0014/ ,0.0029/ ,0.0022/ ,0.00014/ ,0.00075/ ,0.0057/ ,0.0042/ ,0.00027/ ,0.0079/ ,0.001/ ,0.00062/ ,0.00089/ ,6.8e-05/ ,0.0013/ ,0.0066/ ,0.00062/ ,0.00027/ ,0.00089/ ,0.00041/ ,6.8e-05/ ,0.025/ ,0.002/ ,0.00034/ ,0.0066/ ,0.00075/ ,0.0021/ ,0.0011/ ,0.00041/ ,0.0013/ ,0.0014/ ,0.00048/ ,0.0017/ ,0.00075/ ,0.00041/ ,0.0016/ ,0.00041/ ,0.0012/ ,0.001/ ,0.0014/ ,0.00089/ ,0.00068/ ,0.0013/ ,0.00075/ ,0.0019/ ,0.00096/ ,0.00048/ ,0.00041/ ,0.00089/ ,0.0014/ ,6.8e-05/ ,0.00034/ ,0.0018/ ,0.00062/ ,0.001/ ,0.00027/ ,0.00062/ ,0.00048/ ,0.0011/ ,0.00055/ ,0.00096/ ,0.00034/ ,0.00034/ ,0.00027/ ,0.00055/ ,0.0034/ ,0.0016/ ,0.00096/ ,0.0012/ ,0.0025/ ,0.00055/ ,0.0021/ ,6.8e-05/ ,0.00096/ ,0.00068/ ,0.00014/ ,0.0049/ ,0.00075/ ,0.00021/ ,0.00034/ ,0.0017/ ,0.001/ ,0.0015/ ,0.0011/ ,0.00014/ ,0.00014/ ,0.00082/ ,0.00021/ ,0.0012/ ,0.0016/ ,0.00021/ ,0.00021/ ,0.0012/ ,0.0016/ ,0.00075/ ,0.001/ ,0.00068/ ,0.00075/ ,0.00034/ ,0.0016/ ,0.00055/ ,0.00014/ ,0.00014/ ,0.00034/ ,0.00089/ ,0.00021/ ,0.00014/ ,6.8e-05/ ,6.8e-05/ ,0.0035/ ,0.0013/ ,0.00041/ ,6.8e-05/ ,0.00014/ ,0.00048/ ,0.0013/ ,0.00041/ ,0.00021/ ,0.0021/ ,0.00027/ ,6.8e-05/ ,0.00068/ ,0.00014/ ,6.8e-05/ ,0.00082/ ,6.8e-05/ ,0.00041/ ,0.00041/ ,0.00027/ ,0.00068/ ,0.00027/ ,0.00021/ ,0.00048/ ,0.00014/ ,6.8e-05/ ,0.00014/ ,6.8e-05/ ,0.00027/ ,6.8e-05/ ,6.8e-05/ ,0.00014/ ,6.8e-05/ ,0.00041/ ,0.00021/ ,6.8e-05/ ,6.8e-05/ ,6.8e-05/ ,6.8e-05/ ,0.00075/ ,6.8e-05/ ,0.00014/ ,0.00014/ ,6.8e-05/ ,6.8e-05/ ,0.00082/ ,0.0015/ ,0.00041/ ,0.00048/ ,6.8e-05/ ,0.00055/ ,0.00027/ ,0.00014/ ,6.8e-05/ ,0.00027/ ,0.00096/ ,6.8e-05/ ,6.8e-05/ ,0.00021/ ,0.00014/ ,0.00014/ ,0.0011/ ,6.8e-05/ ,0.00014/ ,6.8e-05/ ,6.8e-05/ }
\end{tikzpicture}
  \caption{CERED0}
  \label{fig:corridors_before}
\end{subfigure}
\caption{Corridors. (b) shows how is the second type of configuration spaces used to create space between rooms in the second chain. (c) shows how are corridors added to (b). (d) shows a full layout.}
\label{fig:corridors}
\end{figure}



TACRED






https://www.aclweb.org/anthology/P19-1074.pdf


