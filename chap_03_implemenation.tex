\section{Implementation}

In \autoref{sec:analysis} we discussed different approaches we could choose to solve different aspects of the generation of the dataset. Now we build on the performed analysis and describe in detail the approaches we implemented.


\subsection{Wikidata Preprocessing}
\label{sec:wikidata_preprocessing_implementation}
The goal of Wikidata preprocessing is to load the Wikidata dump file and output three lists. The first list contains the QPQ triples, which are triples of identifiers representing a relationship (Q173998 P3373 Q187923, for example). The second list provides us with the names for the identifiers mentioned in the first list (Q187923 Ginny Weasley, P3373 sibling). The third list contains the mapping of entity identifier to the title of the corresponding Wikipedia article, if such exists. Those three lists serve as the source of structured data.

The first step of our preprocessing is to filter wikidata to remove entities that we will not use. We require each entity to have at least one Czech name (alias or label), otherwise, there would be no good way to find mentions of that entity later in the entity matching step. CERED creation does not require the Czech names of relations, so we will keep all of them.

Apart from the filtering, we should consider, whether we might benefit from keeping more information about a relationship (meaning the instantiation of a relation) than just the identifiers. Wikidata relationships often contain additional information specific for the relation type. For example, the \wikiitem{position held}{P39} relationship between \wikiitem{Cornelius Fudge}{Q1250951} and the \wikiitem{Ministry of Magic}{Q6017614} has following additional information attached to it: \textit{start time 1990, end time 1996, replaces Millicent Bagnold, replaced by Rufus Scrimgeour}. In our use case, given that we plan to limit the text analysis to the syntactic level, such information is not beneficial. 

The second step addresses removing duplicate relationships that differ only in the additional information tied to the relationship (CBS received many Peabody Awards, for example). Such duplicates, where the entire QPQ is the same, might require special attention in the relation matching step. We remove even relationships that differ only in the \vuvozovkach{P} part. If we kept them, in the relation matching we would either create multiple relation mentions (sentence with two tagged entities and the label of their relationship) that only differ in the relationship. However, CERED is designed to be a dataset on which it is possible to train a model for a single-label classification task. The dataset could be extended to allow multi-label relation type classification. However, it would complicate the models and evaluation, and the existing English datasets are also single-label, therefore, we decided to aim for a single-label dataset in this thesis.

After these steps, the first list contains approximately 2 million QPQ triples.

\subsection{Wikitext Parsing}

\label{sec:wikiperia_parsing_implementation}

In this step we aim to parse Wikitext (Wikipedia markup language) from the Czech Wikipedia dump into a clean text with attached information about Wikilinks in the original markup.

As we already explained in \autoref{sec:wikipedia_parsing_analysis}, Wikitext contains a lot more than fully unstructured data. Different kinds of infoboxes, tables or lists are contained within the sentence-like text. Some of these elements are implemented using the so-called template syntax. Therefore, it would be tempting to simply remove all the text that is contained in a template. The problem is that some templates are useful. For example, we may use a template to divide the text into two columns containing valid sentences. Therefore, discarding all such data seems unnecessarily harsh.

When developing the methodology for Wikitext parsing, there was not a \vuvozovkach{gold} output to compare with. The only means of evaluation we had at the time was repeatedly going through a small set of articles and trying to discard unnecessary data. We tailored the rules for Wikitext parsing to these articles in such a way that only sentence-like parts remained. 

Once we implemented the whole CERED generator and were able to see the relation mentions, we realized that the previous method of evaluation was not good enough. Therefore, we adopted a new one, as described in \autoref{sec:analyzaevaluace}. We looked at the different histograms and investigated the abnormalities. For example, a lot of sports articles report results of a match (tournament, event) and these are often stored in custom tables that were not filtered by the rules from the previous paragraph. Moreover, these tables often contain information about the nationality of the players, resulting in a huge amount of matched entities and relations.

Based on the analysis of all the available data, we decided not to include the following content in the clean text:
\begin{itemize}

\item HTML tags within wikitext;
\item headings;
\item tables;
\item lists;
\item templates matching the following patterns: obsazení*, sloupce*, seznam*, příbuzenstvo*, *předkové*, *box*, *locmap*, *tabulka* (the English equivalents are cast*, columns*, lists*, relatives*, *ancestors*, *box*, *locmap*, *table*);
\item wikilinks to categories and files.

\end{itemize}

One more technical issue we encountered was correctly assigning spans to Wikilinks, i.e. where the link starts and ends in the text. We can demonstrate the problem on the  following sentence: 
\begin{quote}
\vuvozovkach{The main [[story arc]] concerns Harry's struggle against [[Lord Voldemort]], a dark wizard who intends to become immortal, overthrow the wizard governing body known as the [[Ministry of Magic]] and subjugate all wizards and [[Muggle]]s (non-magical people).}
\end{quote}
 The correct span for Muggles should contain the trailing s even though it is not part of the Wikilink itself. In Czech such trailing characters are common. The set of characters that seem to end Wikilinks written in such forms are \verb*| ,.\n|.

One more thing we mentioned in the analysis about Wikitext is the possible boost of performance, if the text-part of a Wikilink was added to the set of names for the given entity. We exported such names, kept only those that were not already added to Wikidata, and read through many of them. This process is time-consuming, because one often has to actually look up the entity to know whether a given name is sensible. Even though we do not have any data about the proportion of good and bad names, the overall impression was clearly leaning towards not using such data. The two main reasons were that commonly the name was actually a class name, not instance name (like school linking to Hogwarts), and that pronouns were also linked to corresponding entities (such cases were less frequent, but would likely cause much trouble later on). 

\subsection{Entity Matching}
\label{sec:entity_matching_implementace}
We discussed in great detail the pros and cons of different entity matching methods, implying that the more complex the matching method, the better. We work with a single language and tools for lexical analysis are available and reliable. Therefore, implementing language-independent matching methods (string similarity, for example) is not beneficial. 

We load the entity names in a slightly transformed form -- we lower the case and add spaces around every dot character. We then use a lexical analyzer to split text to sentences and to obtain features from sentences (tokens, lemmas, and lexemes). An entity name (sequence of $k$ tokens separated by a space) is matched in a sentence if (i) the entity is a candidate entity for the sentence, and (ii) there is a sequence of $k$ consecutive tokens in the sentence, such that each token in the name is a member of the lexeme of the corresponding token in the sentence. 

We intended to allow a less strict word order, but we were unable to justify such a choice. After reading several articles, we did not find any entity mentions that would be newly matched. This might imply that even though word order is relatively free in Czech, noun phrases tend to keep their word order. The other explanation is based more on the fact, that a human reader is more likely to recognize an entity mention, if it is in the standard word order. We, therefore, believe that matching entities in any word order would decrease precision more than increase recall, and do not allow it.

We considered allowing one special case. Most articles are based on one entity, therefore we expect many sentences to mention this entity. Often the entity is mentioned either by a pronoun (pronouns that express the subject are typically omitted in Czech), or by part of its full name. We already stated that we ignore pronouns but we tried to propose rules for choosing the correct substring of the entity name. The diversity of Wikidata makes such a task extremely difficult. Together with the risk that we could decrease the precision of entity matching, we decided to stick with full names only.

In the Wikitext parsing section, we prepared spans and identifiers for Wikilinks. We merge these with the ones matched by this module and post-process them -- we discard each mention, whose span is within a span of a different mention of the same entity. This removes duplicates and keeps the one that is the longest.


\subsection{Relation Matching}
So far we obtained sentences with tagged entity mentions. For each tuple of entity mentions within the same sentence, we checked if a relationship of those two entities was present in Wikidata (using the prepared QPQ list). Given the filtering in Wikidata preprocessing, we are guaranteed that there is at most one such relationship. 

At this stage, we need to address incorrectly matched entities that cause the dataset to bloat. One example of such bloating that we encountered was in an article about kindergarten,\footnote{\url{https://cs.wikipedia.org/w/index.php?oldid=18388144}} in the following sentence, where thousands of relation mentions were found. \vuvozovkachtextit{Jsou závazná pro předškolní vzdělávání v \textbf{mateřských školách}, v \textbf{mateřských školách} zřízených podle § 16 odst. 9 školského zákona, v lesních \textbf{mateřských školách} a v přípravných třídách základních škol.}  Many kindergartens are named Mateřská škola (kindergarten), all of them are an instance of the abstract kindergarten entity and therefore candidate entities. If a sentence contains the term \vuvozovkach{mateřská škola} (or its form), all these entities will be matched, therefore, the relationship \vuvozovkach{Mateřská škola is a mateřská škola} will be assigned many times as well. 

After investigating many other unusual cases, we decided to discard any sentence with at least 10 entity mentions in it. We also tried to experiment with different limits, but the results were unconvincing. For example, increasing the constant to 50 keeps an additional 13\% of relationship mentions, but extends the set of sentences only by 1\%.


\subsection{Characteristics of the Generated Dataset}
The full dataset, which was obtained by the process we described in the previous sections, contains almost one and a half million relation mentions. In the next few paragraphs, we present more detailed statistic of this dataset - CERED0. 

We found at least one mention in \num{293591} articles. In these articles, the average number of found mentions is slightly less than 5 and the median is nearly 7. The article with most mentions is Spojené království\footnote{\url{https://cs.wikipedia.org/w/index.php?oldid=18752891}} (United Kingdom). The distribution of mentions in articles is shown in \autoref{fig:kolaceCered0}.

There are \num{490501} different sentences that contain a mentions. We set the upper bound on the number of entity mentions per sentence to 10. On average there were approximately 3 relation mentions in a sentence (that had at least one mention) and the maximum of 72 mentions per sentence was reached 34 times. The full distribution is captured in \autoref{obr:wordsinsentences} (CERED0). The length of sentences ranges from 2 to 401 tokens, where the very short ones usually come from templates that were not removed. On the other hand, the very long ones are often caused by incorrectly written articles.\footnote{Example of an incorrectly written article \url{https://cs.wikipedia.org/w/index.php?oldid=18723498}} We tried to remove all templates to see if the range (and distribution) of the number of words improves, but we did not achieve a significant improvement.




Another possibility is to observe how the position of a sentence in an article influences the number of relation mentions. We expected that the first sentences in the articles would contain the highest number of relation mentions. The first sentences tend to contain Wikilinks and the use of pronouns or shorter names is limited, because each entity has to first be introduced by its full name. As we can see in \autoref{obr:firstsentences} (CERED0), our hypothesis seems to be correct, and partially applicable to several initial sentences, not just the first one. Surprisingly, \num{904803} mentions come from sentences that are first in their respective articles -- this constitutes over 60\% of all mentions.





\section{CERED Versions}
In this section we describe how and why we decided to postprocess the generated datasets. 

The full CERED is already a valid relationship classification dataset. It has nearly one and a half million mentions, but as we discussed in the previous section, some of them might be of poorer quality than others. In this section, we describe different versions of CERED with CERED0 being the biggest (least filtered) and CERED4 the smallest. 


Each dataset version is split up into three disjoint sets: a train set, a dev set and a test set. Ideally, the test set would operate on a different set of entities (so that models learn to predict relationships based on sentences, not on the knowledge of entities). We believe that such a restriction is unnecessarily strong. Some entities are mentioned in many articles, which does, in fact, make them part of common knowledge (connected to the language we train on). Instead, we decided to relax the restriction to distinct articles. We sampled two sets of \num{10000} articles, one for the test set and one for the dev set. In each version of CERED, the same articles are used for dev, test and train.

\subsection{CERED0}
CERED0 is the raw dataset we described in the previous section. We do not artificially change the relation type, so no negative relation is present. We do not recommend this version for direct training, we mostly keep it to preserve the full information obtainable from our generator.

\subsection{CERED1}
CERED1 is close in size to CERED0. We removed long (over 100 words) and short (under 5 words) sentences. We also removed overly represented relations and changed labels of under-represented relations to \relationtype{other}, which is the negative relation in CERED1. We properly define under-represented and over-represented relations in the next paragraph, where we describe the CERED2 dataset.

\subsection{CERED2}

CERED1 has 3 potential flaws that we have not addressed yet. First, one sentence could be included multiple times in the dataset. Second, there are some relations with only a few relation mentions. Third, we did not try to handicap overly represented relatations. 

We try to address these issues in CERED2. We start with CERED1 and for each sentence, if there are multiple relation mentions, we keep only the mention that was the least represented in CERED1, and discard the others. Next, we count the sizes of representations of all mentions and say that a relation is under-represented if it has less than \num{1000} mentions. We take all the mentions whose relation is considered under-represented and relabel them to \relationtype{other}. This change is also applied to CERED1. After such relabeling, the \relationtype{other} relation still was not the most represented. Lastly, we decided to discard mentions of the two most represented relations: \relationtype{\wikiitem{instance of}{P31}} and \relationtype{\wikiitem{country}{P17}}.

\begin{table}

\caption{CERED statistics.}

\label{table:CEREDsStatistics}

\begin{tabular}{p{2,3cm} r r r r }

\hline
Dataset & \# mentions & Inventory size & Negative \% & Test dataset\\
\hline
\hline
CERED0 & \num{1462546} & 692 & 0 & \num{25066}\\
CERED1 & \num{999287} & 64 & 14.1 & \num{17214}\\
CERED2 & \num{374914} & 64 & 18.8 & \num{6530}\\
CERED3 & \num{157972} & 64 & 22.6 & \num{2787}\\
CERED4 & \num{10766} & 59 & 22.1 & 188\\
\hline


\end{tabular}

\end{table}






\subsection{CERED3}
Even after the restriction on the uniqueness of sentences, half of the CERED2 sentences are the first sentences within the articles they originated from. Such sentences have a rather unique structure that is not as common outside of Wikipedia. This inspired CERED3, which is the remainder of CERED2 after removing all mentions in the \vuvozovkach{first} sentences.

\subsection{CERED4}
There are two stages in the CERED generation process that might be relatively unprecise. The entity matching stage and the relation matching stage. CERED4 keeps only the relation mentions from CERED3, in which both entities were manually labelled directly in wikitext in the form of wikilinks. The CERED4 dataset is, unfortunately, small, but has the highest potential to be precise.


\begin{figure}[h]\centering
\includegraphics[scale=1.2]{./img/firstsentence0-4_valid}
\caption{Distribution of the sentence number within article. For CERED0-2, first sentences dominate the datasets. CERED2 and CERED3 differ only in the first sentences, therefore CERED2 is mostly hidden by CERED3.}
\label{obr:firstsentences}
\end{figure}

\begin{figure}[h]\centering
\includegraphics[scale=1.2]{./img/Words0-4_valid}
\caption{todo}
\label{obr:wordsinsentences}
\end{figure}


\subsection{Other Considered Variations}
\label{sec:otherconsideredvariations}
When curating the CERED we considered many different criteria. For example, we chose the constant \num{1000} for underrepresented relations after several experiments. The advantage of this value is that only two relations were more dominant than the new \relationtype{other} relation and at the same time, we would keep 63 positive relations. We wanted our dataset to be at least somewhat comparable to other datasets in the proportion of data and inventory size. S10T8 contains \num{10717} mentions and 10 relations, TACRED contains around \num{100000} mentions and 41 relations. The size of Riedel NYT depends, just like its inventory, on what data is used (and whether duplicates are removed and so on), but commonly used version contains nearly \num{700000} mentions and 58 relations. Since the sizes of CERED1-3 range from approximately a million to approximately \num{160000} mentions, we wanted the relation inventory size to be around 50 or 60. The distributions of relations in different datasets is captured in \autoref{fig:kolace} and detailed for CERED1-4 in \autoref{obr:relations}.


\input{datasets_relation_distribution}




\begin{figure}[h]\centering
\includegraphics[scale=1]{./img/Relations1-4_valid}
\caption{Sizes of relation representations. For each we ordered the relations by their frequency.}
\label{obr:relations}
\end{figure}

\paragraph{Named entities.}

One more labour intensive variation we wanted to create was supposed to be focused on named entities. We tried to curate a set of named entity types (such as person, place, etc.) based on Wikidata, in order to later use them as additional information during training. 

We used the QPQ triples described in \autoref{sec:wikidata_preprocessing_implementation} to create an oriented graph with two types of edges. The first edge type -- instance edges -- connects Wikidata items Q\textsubscript{1} and Q\textsubscript{2} if there is a triple Q\textsubscript{1}P31Q\textsubscript{2}, where P31 is the Wikidata property \wikiitem{instance of}{P31}, and the second type -- subclass edges -- are added for the \wikiitem{subclass of}{P279} property. The goal of the graph is to capture the transitive nature of the two properties, i.e. if Harry Potter is an instance of a fictional human and fictional human is a subclass of a human, we can assume that Harry Potter is a human.\todo{pozor, chyba} We consider a Wikidata item to be a named entity if it is an instance of some other item. If we are interested in all the named entities belonging to some entity type (node in the graph), we can simply find all the named entities for which there is a path between the type and the entity.

We hoped that with such representation, we would be able to curate reasonable set of types. For example, we might want to have a \vuvozovkach{human} entity type and we might intuitively expect \wikiitem{Harry Potter}{Q3244512} (fictional human) to be a human. However, as he is fictional, he is not considered to be human in the wikidata ontology. Such an issue can be solved, for example, by adding another type of edges for \wikiitem{fictional analog of}{P1074}. The problem is that there are just too many similar problems and it is out of scope for this thesis to properly analyze the whole wikidata ontology. 


