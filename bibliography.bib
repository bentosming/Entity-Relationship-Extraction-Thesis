%
%  An example of a bibliographical database for bibTeX
%
%  Recommended software for maintenance of *.bib files:
%    JabRef, http://jabref.sourceforge.net/
%
%  BEWARE:
%
%    *  If a name contains a capital letter, which must be kept such,
%       use curly brackets ({T}hailand, {HIV}).
%
%  ===========================================================================
@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

@inproceedings{nytdistant,
author = {Riedel, Sebastian and Yao, Limin and Mccallum, Andrew},
year = {2010},
month = {09},
pages = {148-163},
title = {Modeling Relations and Their Mentions without Labeled Text},
doi = {10.1007/978-3-642-15939-8_10}
}

@inproceedings{zhang2017tacred,
  author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D.},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017)},
  title = {Position-aware Attention and Supervised Data Improve Slot Filling},
  url = {https://nlp.stanford.edu/pubs/zhang2017tacred.pdf},
  pages = {35--45},
  year = {2017}
}


@inproceedings{semeval,
    title = "{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals",
    author = "Hendrickx, Iris  and
      Kim, Su Nam  and
      Kozareva, Zornitsa  and
      Nakov, Preslav  and
      {\'O} S{\'e}aghdha, Diarmuid  and
      Pad{\'o}, Sebastian  and
      Pennacchiotti, Marco  and
      Romano, Lorenza  and
      Szpakowicz, Stan",
    booktitle = "Proceedings of the 5th International Workshop on Semantic Evaluation",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S10-1006",
    pages = "33--38",
}

@misc{ wiki:wiki,
    author = "{Wikipedia contributors}",
    title = "Wikipedia --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2020",
    url = "https://en.wikipedia.org/w/index.php?title=Wikipedia&oldid=947302871",
    note = "[Online; accessed 28-March-2020]"
  }
  
  @InProceedings{Morphodita,
  author    = {Strakov\'{a}, Jana  and  Straka, Milan  and  Haji\v{c}, Jan},
  title     = {Open-{S}ource {T}ools for {M}orphology, {L}emmatization, {POS} {T}agging and {N}amed {E}ntity {R}ecognition},
  booktitle = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland},
  publisher = {Association for Computational Linguistics},
  pages     = {13--18},
  url       = {http://www.aclweb.org/anthology/P/P14/P14-5003.pdf}
}



@article{Opitz2019MacroFA,
  title={Macro F1 and Macro F1},
  author={Juri Opitz and Sebastian Burst},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.03347}
}

@misc{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{mikolov2013word2vec,
    title={Efficient Estimation of Word Representations in Vector Space},
    author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
    year={2013},
    eprint={1301.3781},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{baldini-soares-etal-2019-matching,
    title = "Matching the Blanks: Distributional Similarity for Relation Learning",
    author = "Baldini Soares, Livio  and
      FitzGerald, Nicholas  and
      Ling, Jeffrey  and
      Kwiatkowski, Tom",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1279",
    doi = "10.18653/v1/P19-1279",
    pages = "2895--2905",
    abstract = "General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris{'} distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task{'}s training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED",
}

@book{linguistic2008new,
  title={The New York Times Annotated Corpus},
  author={Linguistic Data Consortium and New York Times Company},
  series={LDC corpora},
  url={https://books.google.cz/books?id=D4F2AQAACAAJ},
  year={2008},
  publisher={Linguistic Data Consortium}
}

@INPROCEEDINGS{Bollacker08freebase,
    author = {Kurt Bollacker and Colin Evans and Praveen Paritosh and Tim Sturge and Jamie Taylor},
    title = {Freebase: a collaboratively created graph database for structuring human knowledge},
    booktitle = {In SIGMOD Conference},
    year = {2008},
    pages = {1247--1250}
}

@inproceedings{lin-etal-2016-neural,
    title = "Neural Relation Extraction with Selective Attention over Instances",
    author = "Lin, Yankai  and
      Shen, Shiqi  and
      Liu, Zhiyuan  and
      Luan, Huanbo  and
      Sun, Maosong",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1200",
    doi = "10.18653/v1/P16-1200",
    pages = "2124--2133",
}

@article{DBLP:journals/corr/abs-1905-08284,
  author    = {Shanchan Wu and
               Yifan He},
  title     = {Enriching Pre-trained Language Model with Entity Information for Relation
               Classification},
  journal   = {CoRR},
  volume    = {abs/1905.08284},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.08284},
  archivePrefix = {arXiv},
  eprint    = {1905.08284},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-08284.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1901-08163,
  author    = {Joohong Lee and
               Sangwoo Seo and
               Yong Suk Choi},
  title     = {Semantic Relation Classification via Bidirectional {LSTM} Networks
               with Entity-aware Attention using Latent Entity Typing},
  journal   = {CoRR},
  volume    = {abs/1901.08163},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.08163},
  archivePrefix = {arXiv},
  eprint    = {1901.08163},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-08163.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhang-etal-2018-graph-TACRED,
    title = "Graph Convolution over Pruned Dependency Trees Improves Relation Extraction",
    author = "Zhang, Yuhao  and
      Qi, Peng  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1244",
    doi = "10.18653/v1/D18-1244",
    pages = "2205--2215",
    abstract = "Dependency trees help relation extraction models capture long-range relations between words. However, existing dependency-based models either neglect crucial information (e.g., negation) by pruning the dependency trees too aggressively, or are computationally inefficient because it is difficult to parallelize over different tree structures. We propose an extension of graph convolutional networks that is tailored for relation extraction, which pools information over arbitrary dependency structures efficiently in parallel. To incorporate relevant information while maximally removing irrelevant content, we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold. The resulting model achieves state-of-the-art performance on the large-scale TACRED dataset, outperforming existing sequence and dependency-based neural models. We also show through detailed analysis that this model has complementary strengths to sequence models, and combining them further improves the state of the art.",
    }
    
@inproceedings{zhang-etal-2017-position_TACRED,
    title = "Position-aware Attention and Supervised Data Improve Slot Filling",
    author = "Zhang, Yuhao  and
      Zhong, Victor  and
      Chen, Danqi  and
      Angeli, Gabor  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1004",
    doi = "10.18653/v1/D17-1004",
    pages = "35--45",
    abstract = "Organized relational knowledge in the form of {``}knowledge graphs{''} is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2{\%} to 26.7{\%}.",
}

@unknown{nyt-nefunguje,
author = {Moreira, Johny and Oliveira, Chaina and Macedo, David and Zanchettin, Cleber and Barbosa, Luciano},
year = {2020},
month = {04},
pages = {},
title = {Distantly-Supervised Neural Relation Extraction with Side Information using BERT}
}